\documentclass[11pt, a4paper, oneside]{article}

% hifenização e outras especificações para português
\usepackage[portuguese]{babel}

% hiperligações
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=black}

% escrever acentos e coisas do género sem que o latex se desoriente
\usepackage[utf8]{inputenc}

% para ter imagens, depois define a directoria de imagens
\usepackage{graphicx}
\graphicspath{{./imagens/}}

\usepackage[labelformat=simple]{caption}
\usepackage[labelformat=empty]{subcaption}

% para ter a informação de quantas páginas tem o documento
\usepackage{lastpage}

% definir o cabeçalho e rodapé
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{\small{Processamento da Wikipédia}}
\fancyhead[R]{\small{Processamento de Linguagens}}

% ter enumerações alinhadas
\usepackage{enumitem}

% escrever algoritmos
\usepackage[algoruled]{algorithm2e}

% mais cores predefinidas
\usepackage[usenames,dvipsnames]{color}

% definir comandos especiais
\newcommand\doubleplus{+\kern-1.3ex+\kern0.8ex} %

\newcommand{\todo}[1] {\textcolor{BrickRed}{\begin{quote}#1\end{quote}}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% inicio do documento
\begin{document}
\title{Processamento da Wikipédia\\
\begin{normalsize}
Variante 1
\end{normalsize}}
\date{\today\\Universidade do Minho}
\author{
  Bruno Ferreira\\
  {\small A61055}\\
  \and
  Cláudia Oliveira\\
  {\small A60987}\\
  \and
  Vanessa Campos\\
  {\small A54801}\\
}

\maketitle
\begin{abstract}

  O presente trabalho foi desenvolvido no âmbito da Unidade Curricular de Processamento de Linguagens e tem como principal objectivo descrever o processo de desenvolvimento de um analisador léxico da \textit{Wikipédia}. Ao longo deste relatório iremos explicar as estruturas de dados que foram implementadas para o desenvolvimento deste trabalho, bem como todas as decisões que foram tomadas.

\end{abstract}
\newpage

\tableofcontents
\listoffigures 

\newpage
\section{Introdução}

Atualmente a \textit{Wikipédia} é a maior base de dados de conhecimento livre online e disponível em várias línguas, no qual a lingua portuguesa faz parte integrante. Neste contexto, pretende-se desenvolver um Processador de Texto, exportado do \textit{Wikipédia} e gerar a página html com os dados mais relevantes contidos nessa página, utilizando a ferramenta Flex. 
Com este trabalho foi necessário implementar Expressões Regulares de modo a retirar informação mais relevante num contexto enorme de informção, encontrando padrões que nos ajudem a descobrir o tipo de informação que estamos a procura e que é útil no contexto deste caso de estudo.


\newpage
\section{Desenvolvimento}

\subsection{Contextualização do Problema}

Flex é uma ferramenta para gerar automáticamente analisadores léxicos, isto é, programas que reconhecem padrões léxicos num texto. O Flex é uma evolução da ferramenta Lex, mas com a caraterística de ser mais rápido que este (Fast Lex). Lex foi desenvolvido por M.E. Lesk e E. Shmidt (Bell Laboratories - ATT) enquanto que o Flex é um produto da Free Software Foundation, Inc. Como são comumente distribuídos em sistemas Unix a sua documentação encontra-se na forma de manual pages para as entradas lex, flex e flexdoc. Ao contrário do programador que escreve manualmene um programa que realize a identificação de padrões numa entrada, o uso do Flex/Lex permite que sejam apenas especificados os padrões desejados e as ações necessárias para processá-los. Para que Flex/Lex reconheçam padrões no texto, tais padrões devem ser descritos através de expressões regulares. A ferramenta Flex é ótima para a realização deste trabalho uma vez que nos permite analisar o contéudo específico, através de expressões regulares de determinada informação de texto.

\subsection{Processamento da \textit{Wikipédia}}
Neste primeiro trabalho de Processamento de Linguagens foi-nos proposto escolher um tema a escolha, de vários enunciados propostos. Após uma análise cuidada  de cada um deles o nosso grupo optou por escolher o tema de Processamento da \textit{Wikipédia}. Esta escolha reflete-se pelo grande impacto que a \textit{Wikipédia} tem no nosso dia-a-dia e é a principal enciclopédia livre com mais utilizadores. 
O objectivo deste enunciado é criar um programa utilizando a ferramenta Flex que analise o contéudo de uma página XML da \textit{Wikipédia}, e que retire as informações relevantes dessa página como:
\begin{itemize}
\item título,
\item autor.
\item data
\item n.º de links internos
\item n.º de links externos
\item n.º de sessões 
\end{itemize}
e criar uma página HTML (para cada página XML) com essas informações. Para isso é necessário exportar uma ou mais páginas usando o \textit{Special Export} disponível em \textbf{http://pt.wikipedia.org/wiki/Especial:Exportar (ou \\http://en.wikipedia.org/wiki/Special:Export} para a versão inglesa.
\newpage

\subsection{Enunciado}

De uma maneira geral, para teste trabalhou pretende-se desenvolver os seguintes pontos:
\begin{itemize}
\item Especificar padrões de frases que se quer encontrar no texto fonte através de ERs.
\item Identificar as acções semânticas a realizar como reação ao reconhecimento de cada um desses padrões
\item Identificar estruturas de dados globais que possa eventualmente precisar para armazenar temporariamente a informação que se vai extraindo ou que se vai construindo à medida que o processamento avança.
\item Desenvolver um Processador de Texto para fazer reconhecimento dos padrões identificados e proceder à transformação pretendida, com recurso do Flex.
\end{itemize}

\subsection{Descrição do Problema}

Usando o \textbf{Especial Export} pretende-se exportar o contéudo da \textit{Wikipédia} e com o recurso do Flex implementar um processador de texto que deverá produzir, por cada página extraída do ficheiro XML, uma página \textbf{html} com as seguintes informações:

\begin{itemize}
\item Título
\item Autor da última revisão
\item Data da última revisão
\item Número de links internos, e explicitar quais.
\item Número de links externos, e explicitar quais.
\item Número de seções, e explicitar quais.
\item Opcional: mostrar na página \textit{html} o contéudo original do artigo.
\end{itemize}

\newpage

\subsection{Desenvolvimento do Programa}

\subsubsection{Estrutura de Dados}


\subsubsection{Flex e Expressões Regulares}

Nesta secção iremos fazer uma abordagem as Expressões Regulares que foram desenvolvidas no contexto da resolução deste projecto. Utilizaram-se as seguintes variáveis de contexto:

\begin{verbatim}
%x page
%x title
%x revision
%x timestamp
%x contributor
%x username
%x text
\end{verbatim}

A cada uma das expressões regulares apresentadas será feito uma breve descrição de cada uma delas:
\begin{itemize}
\item 
\begin{verbatim}
"<page>"           {pag = pagina_create();
                    printf("inicio nova pagina\n");
                    BEGIN page;
                    }

<page>"</page>"    {printf("fim pagina\n");
                    BEGIN 0;
                    }
\end{verbatim}

Aqui é guardado o início e o fim da página da leitura do ficheiro XML. Quando se encontra a tag \begin{math}<page>\end{math} inicia a estrutura page. Por outro lado, quando encontra \begin{math}</page>\end{math}, volta ao estado inicial e escreve a página HTML com toda a informação filtrada até então.



\end{itemize}


\newpage
\section{Conclusão}

\newpage

\todo{pode-se utilizar esta tag para chamar a atenção para coisas que é preciso fazer antes de entregar o relatório}

\newpage
\section{Elementos do Grupo}
\begin{figure}[h!]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{60}
  \caption{Bruno Ferreira  }
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{107}
  \caption{Cláudia Oliveira}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{93}
  \caption{Vanessa Campos}
\end{subfigure}%
\end{figure}


\end{document}